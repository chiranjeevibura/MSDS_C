{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e454d051",
   "metadata": {},
   "source": [
    "\n",
    "# RAG Proof of Concept for Contract Documents using BM25 Sparse Vectors\n",
    "\n",
    "In this notebook, we will build a Retrieval-Augmented Generation (RAG) system using BM25 sparse vectors for document chunk retrieval. The system will work on contract documents, allowing us to extract relevant chunks based on a user's query.\n",
    "\n",
    "We will implement the following steps:\n",
    "1. **Document Ingestion**: Extract text from PDF contract documents.\n",
    "2. **Chunking**: Split the document text into smaller, manageable chunks.\n",
    "3. **Indexing with BM25**: Index the chunks using BM25 for efficient retrieval.\n",
    "4. **Querying**: Retrieve the top N relevant document chunks for a given query using BM25 scores.\n",
    "5. **Output**: Display the results.\n",
    "\n",
    "By the end of this notebook, you will have a working document retrieval system that can be extended for future improvements, such as using LLM-based embeddings for even better performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2b2f13",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2: Reading PDF Contract Documents\n",
    "\n",
    "In this step, we will use the `PyMuPDF` library to extract text from PDF contract documents. PyMuPDF provides a simple way to open PDF files and extract text from each page. This allows us to load the contents of a contract document for further processing.\n",
    "\n",
    "We will define a function `extract_text_from_pdf` that accepts the file path of a PDF and returns the extracted text.\n",
    "\n",
    "### Code to Extract Text from PDFs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d2a469",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Function to extract text from a PDF document\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)  # Open the PDF file\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()  # Extract text from each page\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "pdf_path = \"path_to_contract_document.pdf\"\n",
    "document_text = extract_text_from_pdf(pdf_path)\n",
    "print(document_text[:500])  # Print the first 500 characters of the extracted text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d836417",
   "metadata": {},
   "source": [
    "\n",
    "## Step 3: Chunking the Document Text\n",
    "\n",
    "After extracting the text from the PDF contract document, we need to split the document into smaller chunks. Chunking helps ensure that we can process and query the document efficiently. We will split the document into chunks of a fixed size (e.g., 500 characters).\n",
    "\n",
    "The function `chunk_document` takes the entire document text and splits it into smaller chunks, each containing a maximum of 500 characters.\n",
    "\n",
    "### Code to Chunk Document Text:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7603ec4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to chunk document text into fixed-size chunks\n",
    "def chunk_document(text, chunk_size=500):\n",
    "    # Split text into chunks of specified size (e.g., 500 characters per chunk)\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# Example usage to chunk the extracted document text\n",
    "chunks = chunk_document(document_text)\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(f\"First chunk: {chunks[0][:100]}...\")  # Print first 100 characters of the first chunk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825e2479",
   "metadata": {},
   "source": [
    "\n",
    "## Step 4: Storing Document Chunks in MongoDB\n",
    "\n",
    "Once we have chunked the document text, we need to store these chunks in a database for easy retrieval. MongoDB is an ideal choice for this task, as it offers a flexible, scalable storage solution.\n",
    "\n",
    "We will store the chunks in a collection called `chunks` within a MongoDB database `contract_documents`. Each chunk will be associated with its document ID and chunk text.\n",
    "\n",
    "### Code to Store Document Chunks in MongoDB:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bbad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pymongo\n",
    "from bson import ObjectId\n",
    "\n",
    "# Establish MongoDB connection\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client['contract_documents']\n",
    "collection = db['chunks']\n",
    "\n",
    "# Function to store document chunks in MongoDB\n",
    "def store_document_chunks(doc_id, document_text):\n",
    "    chunks = chunk_document(document_text)\n",
    "    chunk_data = [{'document_id': doc_id, 'text': chunk} for chunk in chunks]\n",
    "    collection.insert_many(chunk_data)\n",
    "\n",
    "# Example usage\n",
    "doc_id = ObjectId()  # Generate a unique document ID\n",
    "store_document_chunks(doc_id, document_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ff6efc",
   "metadata": {},
   "source": [
    "\n",
    "## Step 5: BM25 Indexing for Document Chunks\n",
    "\n",
    "Now that we have stored the document chunks in MongoDB, we need to index them using BM25. BM25 is a popular ranking function used for information retrieval. It ranks documents based on term frequency and inverse document frequency (TF-IDF), allowing us to identify relevant chunks for a given query.\n",
    "\n",
    "We will use the `rank_bm25` library to build the BM25 index. This library computes BM25 scores for each chunk in the document corpus.\n",
    "\n",
    "### Code to Create BM25 Index:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd0fa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "\n",
    "# Function to calculate BM25 embeddings for document chunks\n",
    "def create_bm25_index():\n",
    "    # Fetch chunks from MongoDB\n",
    "    chunks = list(collection.find())\n",
    "    corpus = [chunk['text'] for chunk in chunks]\n",
    "    \n",
    "    # Tokenize the corpus for BM25\n",
    "    tokenized_corpus = [doc.split() for doc in corpus]\n",
    "    \n",
    "    # Create BM25 index\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    \n",
    "    return bm25, chunks\n",
    "\n",
    "bm25_index, chunks_data = create_bm25_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c218fe",
   "metadata": {},
   "source": [
    "\n",
    "## Step 6: Querying for Relevant Document Chunks\n",
    "\n",
    "With the BM25 index created, we can now query the document chunks to retrieve the most relevant ones for a given search query. We will define a function `query_bm25` that accepts a search query, computes BM25 scores for all document chunks, and returns the top N most relevant chunks.\n",
    "\n",
    "The query will be tokenized, and the BM25 scores will be calculated to determine the relevance of each chunk to the query.\n",
    "\n",
    "### Code to Query BM25:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14521c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to query the BM25 index\n",
    "def query_bm25(query, bm25, chunks_data, top_n=5):\n",
    "    tokenized_query = query.split()  # Tokenize the query\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    # Get top N relevant chunks based on BM25 score\n",
    "    top_n_indexes = np.argsort(scores)[-top_n:][::-1]\n",
    "    \n",
    "    relevant_chunks = []\n",
    "    for index in top_n_indexes:\n",
    "        chunk = chunks_data[index]\n",
    "        relevant_chunks.append({'document_id': chunk['document_id'], 'text': chunk['text'], 'score': scores[index]})\n",
    "    \n",
    "    return relevant_chunks\n",
    "\n",
    "# Example query\n",
    "query = \"contract payment terms\"\n",
    "relevant_chunks = query_bm25(query, bm25_index, chunks_data)\n",
    "for chunk in relevant_chunks:\n",
    "    print(f\"Document ID: {chunk['document_id']}, Score: {chunk['score']}, Text: {chunk['text'][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfffbd49",
   "metadata": {},
   "source": [
    "\n",
    "## Step 7: Outputting Results to the User\n",
    "\n",
    "Finally, we will display the relevant document chunks to the user. We will show the top N relevant chunks, including their BM25 score, document ID, and a snippet of the chunk text.\n",
    "\n",
    "This step ensures that the user can easily access the relevant parts of the contract document based on their query.\n",
    "\n",
    "### Code to Display Results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febc6600",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display top N relevant chunks to the user\n",
    "def display_relevant_chunks(chunks):\n",
    "    for chunk in chunks:\n",
    "        print(f\"Document ID: {chunk['document_id']}, Score: {chunk['score']}, Text: {chunk['text'][:150]}...\")\n",
    "\n",
    "display_relevant_chunks(relevant_chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967e0c62",
   "metadata": {},
   "source": [
    "\n",
    "## Step 8: Future Extension to LLM Embeddings\n",
    "\n",
    "In the future, we can extend this system to use embeddings generated by a large language model (LLM) instead of BM25. By using LLMs, we can capture more nuanced semantic meaning in both the query and document chunks.\n",
    "\n",
    "We can generate embeddings for the document chunks and the user query, and then compute cosine similarity between them to retrieve the most relevant document chunks. This approach is more sophisticated and could yield better results for complex queries.\n",
    "\n",
    "### LLM-based Retrieval Pseudocode:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12ae512",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Placeholder for LLM embedding retrieval\n",
    "# Embed document chunks and user query using LLM\n",
    "# Compute cosine similarity between query embedding and document chunk embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33c810a",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "In this notebook, we built a basic Retrieval-Augmented Generation (RAG) system using BM25 for document retrieval. We:\n",
    "1. Extracted text from contract PDF documents.\n",
    "2. Split the documents into chunks and stored them in MongoDB.\n",
    "3. Indexed the chunks using BM25 for efficient retrieval.\n",
    "4. Implemented a query function to retrieve relevant chunks based on BM25 scores.\n",
    "\n",
    "This system can easily be extended in the future to use LLM-based embeddings for better retrieval and semantic understanding. The notebook provides a solid foundation for building a scalable and efficient document retrieval system.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
